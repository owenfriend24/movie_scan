{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "279a695f-2e4c-4c24-8a05-7ffb6d63d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from movie_pkg.llm_human_agreement import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d94b2-4751-471c-a11f-5922fe132f70",
   "metadata": {},
   "source": [
    "### Differences in proportion agreement\n",
    "* here, we use proportion agreement as our primary alignment metric rather than Cohen’s Kappa because the data were highly unbalanced (i.e., events were far more often not described than described), a condition known to distort κ estimates.",
    "* moreover, because both mappings and omissions were central to our research question, we required a metric that would not be differentially biased toward either outcome under class imbalance (Byrt et al., 1993; Friend et al., 2023)",
    "* this measure could also be reported as prevalance-adjusted bias-adjusted Kappa (PABAK = prop.agreement/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892e0ac-921b-494c-bb6e-7454c6b83113",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prim_coder = '' # path to your primary coder's codes\n",
    "base_sec_coder = '' # path to your secondary coder's codes\n",
    "base_llm = '' # path to the llm-generated codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee272d7-09fc-487f-bf75-c4cde3f9cc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human-LLM agreement: 0.8735606947697111, range: 0.75-0.9861111111111112, SD: 0.06916085144099864\n",
      "Human-Human agreement: 0.9025744860785843, range: 0.8555327868852459-0.9508196721311475, SD: 0.0255615060270125\n"
     ]
    }
   ],
   "source": [
    "human_llm_scores = []\n",
    "human_human_scores = []\n",
    "\n",
    "for subject in get_reliability_subjects():\n",
    "    sub_hl_scores = []\n",
    "    sub_hh_scores = []\n",
    "\n",
    "    for movie in get_rel_subject_movies(subject):\n",
    "        \n",
    "        human_llm_score = compute_proportion_agreement_csv_tsv(\n",
    "            f\"{base_prim_coder}/{subject}_{movie}.csv\",\n",
    "            f\"{base_llm}/temple{subject}/subj-temple{subject}_{movie}_event_coded.tsv\",\n",
    "            verbose=False, flexible_match=False, fuzzy=True) # allow fuzzy matching to account for differences in punctuation, capitalization, etc.\n",
    "        \n",
    "        human_human_score = compute_proportion_agreement_csv(\n",
    "            f\"{base_prim_coder}/{subject}_{movie}.csv\",\n",
    "            f\"{base_sec_coder}/{subject}_{movie}.csv\",\n",
    "            verbose=False, flexible_match=False, fuzzy=True)\n",
    "        \n",
    "        sub_hl_scores.append(human_llm_score)  \n",
    "        sub_hh_scores.append(human_human_score)  \n",
    "        \n",
    "    human_llm_scores.append(np.mean(sub_hl_scores))\n",
    "    human_human_scores.append(np.mean(sub_hh_scores))\n",
    "\n",
    "print()\n",
    "print(f'Human-LLM agreement: {np.mean(human_llm_scores)}, range: {min(human_llm_scores)}-{max(human_llm_scores)}, SD: {np.std(human_llm_scores)}')\n",
    "print(f'Human-Human agreement: {np.mean(human_human_scores)}, range: {min(human_human_scores)}-{max(human_human_scores)}, SD: {np.std(human_human_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8177a72-0391-4f49-ba82-6a1a4bdc720f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t(26.0) = 1.4187697199983234, p = 0.1678431885499739\n"
     ]
    }
   ],
   "source": [
    "hh = np.array(human_human_scores)\n",
    "hl = np.array(human_llm_scores)\n",
    "\n",
    "res = stats.ttest_ind(hh, hl)\n",
    "\n",
    "t_stat = res.statistic\n",
    "p_val = res.pvalue\n",
    "df = res.df\n",
    "\n",
    "print(f't({df}) = {t_stat}, p = {p_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b23aaa-d5d8-4619-b2a1-4e50b3723701",
   "metadata": {},
   "source": [
    "### Ensure no differences in automated coding performance based on subject's age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d7ca9f-119b-401e-a39a-3539e3a55234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human-human by age: r = -0.3022042466671977, p = 0.29365740606795876\n",
      "human-llm by age: r = -0.3804903709110294, p = 0.17957081454261997\n"
     ]
    }
   ],
   "source": [
    "ages = np.array(get_rel_subject_ages())\n",
    "\n",
    "r_h, p_val_h = stats.pearsonr(ages, hh)\n",
    "r_l, p_val_l = stats.pearsonr(ages, hl)\n",
    "\n",
    "print(f'human-human by age: r = {r_h}, p = {p_val_h}')\n",
    "print(f'human-llm by age: r = {r_l}, p = {p_val_l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfd842-63a3-43d8-b220-36d6e70307cf",
   "metadata": {},
   "source": [
    "### Characterize disagreements between human-human and human-llm coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e4d13-10d1-4a21-9ce6-0d72f341177d",
   "metadata": {},
   "source": [
    "**Correlation between what movie events lead to disagreements:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b16d315-ffe6-4dbb-bdf4-53a36c5dcc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Fisher z: 0.435\n",
      "Mean Pearson r: 0.409\n",
      "Group-level t = 9.306, p = 0.0000\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for sub in get_reliability_subjects():\n",
    "    for movie in get_rel_subject_movies(subject):\n",
    "        file1 = f\"{base_prim_coder}/{sub}_{movie}.csv\"\n",
    "        file2 = f\"{base_sec_coder}/{sub}_{movie}.csv\"\n",
    "        file3 = f\"{base_llm}/temple{sub}/subj-temple{sub}_{movie}_event_coded.json\"\n",
    "\n",
    "        try:\n",
    "            result = mismatch_correlation(str(file1), str(file2), str(file3), verbose=False)\n",
    "            result.update({\"subject\": sub, \"movie\": movie})\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping subj-{sub}, movie-{movie}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df_clean = df.dropna(subset=['fisher_z'])\n",
    "\n",
    "# compute mean Fisher z and Pearson r\n",
    "mean_fisher_z = df_clean['fisher_z'].mean()\n",
    "mean_r = tanh(mean_fisher_z)\n",
    "\n",
    "print(f\"\\nMean Fisher z: {mean_fisher_z:.3f}\")\n",
    "print(f\"Mean Pearson r: {mean_r:.3f}\")\n",
    "\n",
    "# group-level significance test\n",
    "fisher_zs = df_clean['fisher_z'].values\n",
    "t_stat, p_val = ttest_1samp(fisher_zs, 0)\n",
    "print(f\"Group-level t = {t_stat:.3f}, p = {p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef639f9a-5c28-4689-b562-47b23b335760",
   "metadata": {},
   "source": [
    "**Mismatch distance: how far apart are events that human-human coders disagreed on from human-llm disagreements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bb303d1-aa89-4e3f-b0f0-cc07bb63d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_scores = []\n",
    "llm_scores = []\n",
    "\n",
    "for subject in get_reliability_subjects():\n",
    "    sub_scores = []\n",
    "    llm_sub_scores = []\n",
    "    for movie in get_rel_subject_movies(subject):\n",
    "        file1 = f\"{base_prim_coder}/{subject}_{movie}.csv\"\n",
    "        file2 = f\"{base_sec_coder}/{subject}_{movie}.csv\"\n",
    "        file3 = f\"{base_llm}/temple{subject}/subj-temple{subject}_{movie}_event_coded.json\"\n",
    "\n",
    "        try:\n",
    "            score = compute_human_human_mismatch_distance(file1, file2, verbose=False)\n",
    "            #print(\"Human–Human average mismatch distance:\", score)\n",
    "            sub_scores.append(score)\n",
    "\n",
    "            llm_score = compute_human_llm_mismatch_distance(file1, file3, verbose=False)\n",
    "            llm_sub_scores.append(llm_score)\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"        Skipping missing: {subject}, {movie}: {e}\")\n",
    "            sub_scores.append(np.nan)\n",
    "    human_scores.append(np.nanmean(sub_scores))\n",
    "    llm_scores.append(np.nanmean(llm_sub_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47900330-15b4-4c00-9bf0-f67b65563585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson r = 0.506, p = 0.0647\n"
     ]
    }
   ],
   "source": [
    "x = np.array(human_scores)\n",
    "y = np.array(llm_scores)\n",
    "\n",
    "\n",
    "r, p = pearsonr(x, y)\n",
    "print(f\"Pearson r = {r:.3f}, p = {p:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
